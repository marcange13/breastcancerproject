{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb208045-f6fd-4d9e-851b-32cc688418e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Shape: (569, 31)\n",
      "Columns: ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'diagnosis']\n",
      "\n",
      "Important features (correlation > 0.5): ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'perimeter_worst', 'area_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst']\n",
      "\n",
      "Data split shapes:\n",
      "Full: Train: (455, 30) Val: (57, 30) Test: (57, 30)\n",
      "Reduced: Train: (455, 15) Val: (57, 15) Test: (57, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marca\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\marca\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marca\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\marca\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\marca\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best k for KNN: 4 with cross-val accuracy: 0.9626\n",
      "\n",
      "Classification Results:\n",
      "\n",
      "KNN:\n",
      "Full Features - Accuracy: 0.9474, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 2 15]]\n",
      "Reduced Features - Accuracy: 0.9474, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 2 15]]\n",
      "\n",
      "Random Forest:\n",
      "Full Features - Accuracy: 0.9649, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 1 16]]\n",
      "Reduced Features - Accuracy: 0.9649, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 1 16]]\n",
      "\n",
      "SVC:\n",
      "Full Features - Accuracy: 0.9649, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 1 16]]\n",
      "Reduced Features - Accuracy: 0.9474, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 2 15]]\n",
      "\n",
      "Top 10 features by Random Forest importance: ['area_worst', 'concave points_worst', 'radius_worst', 'concave points_mean', 'perimeter_worst', 'perimeter_mean', 'concavity_mean', 'area_mean', 'radius_mean', 'concavity_worst']\n",
      "\n",
      "Data split shapes (Top 10 features):\n",
      "Train: (455, 10) Val: (57, 10) Test: (57, 10)\n",
      "\n",
      "Classification Results with Top 10 Features:\n",
      "KNN - Accuracy: 0.9474, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 2 15]]\n",
      "Random Forest - Accuracy: 0.9649, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 1 16]]\n",
      "SVC - Accuracy: 0.9649, Confusion Matrix:\n",
      "[[39  1]\n",
      " [ 1 16]]\n",
      "\n",
      "Comparison: Look at accuracy scores above. Full features may perform better due to more information, but reduced sets can achieve >94% accuracy with fewer features, improving efficiency.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load the previously pre-processed dataset\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\marca\\Downloads\\data_refined.csv\")\n",
    "print(\"Dataset loaded. Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Step 2: Feature Selection\n",
    "# Encode the target 'Diagnosed' column ('M' -> 1, 'B' -> 0)\n",
    "le = LabelEncoder()\n",
    "df['Diagnosed'] = le.fit_transform(df['diagnosis'])  # Assuming 'diagnosis' is the column name in the data\n",
    "\n",
    "# Calculate correlation of all features to the target\n",
    "X_full = df.drop(columns=['diagnosis', 'Diagnosed'])  # All features (excluding target)\n",
    "y = df['Diagnosed']\n",
    "correlations = X_full.corrwith(y).abs()  # Absolute correlation with target\n",
    "\n",
    "# Choose features with correlation > 0.5\n",
    "threshold = 0.5\n",
    "important_features = correlations[correlations > threshold].index.tolist()\n",
    "print(\"\\nImportant features (correlation > 0.5):\", important_features)\n",
    "\n",
    "# Create reduced feature set\n",
    "X_reduced = X_full[important_features]\n",
    "\n",
    "# Step 3: Splitting the Data\n",
    "# Split into 80% train, 10% validation, 10% test\n",
    "# First split: 90% train+val, 10% test for both full and reduced sets\n",
    "train_idx, test_idx = train_test_split(range(len(y)), test_size=0.1, random_state=42)\n",
    "X_train_full = X_full.iloc[train_idx]\n",
    "X_test_full = X_full.iloc[test_idx]\n",
    "X_train_reduced = X_reduced.iloc[train_idx]\n",
    "X_test_reduced = X_reduced.iloc[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test = y.iloc[test_idx]\n",
    "\n",
    "# Second split: From 90% (train+val), split into 80% train and 10% val (0.1111 of 90% = 10% of total)\n",
    "train_idx_2, val_idx = train_test_split(range(len(y_train)), test_size=0.1111, random_state=42)\n",
    "X_train_full_final = X_train_full.iloc[train_idx_2]\n",
    "X_val_full = X_train_full.iloc[val_idx]\n",
    "X_train_reduced_final = X_train_reduced.iloc[train_idx_2]\n",
    "X_val_reduced = X_train_reduced.iloc[val_idx]\n",
    "y_train_final = y_train.iloc[train_idx_2]\n",
    "y_val = y_train.iloc[val_idx]\n",
    "\n",
    "print(\"\\nData split shapes:\")\n",
    "print(\"Full: Train:\", X_train_full_final.shape, \"Val:\", X_val_full.shape, \"Test:\", X_test_full.shape)\n",
    "print(\"Reduced: Train:\", X_train_reduced_final.shape, \"Val:\", X_val_reduced.shape, \"Test:\", X_test_reduced.shape)\n",
    "\n",
    "# Step 4: Training Classifiers\n",
    "# a) KNN with cross-validation to find optimal k\n",
    "k_values = range(1, 21)\n",
    "k_scores = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_full_final, y_train_final, cv=5)  # 5-fold cross-validation\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "best_k = k_values[np.argmax(k_scores)]\n",
    "print(f\"\\nBest k for KNN: {best_k} with cross-val accuracy: {max(k_scores):.4f}\")\n",
    "\n",
    "# Train and evaluate classifiers on full and reduced sets\n",
    "classifiers = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=best_k),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVC\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nClassification Results:\")\n",
    "for name, clf in classifiers.items():\n",
    "    # Full features\n",
    "    clf.fit(X_train_full_final, y_train_final)\n",
    "    y_pred_full = clf.predict(X_test_full)\n",
    "    acc_full = accuracy_score(y_test, y_pred_full)\n",
    "    cm_full = confusion_matrix(y_test, y_pred_full)\n",
    "    \n",
    "    # Reduced features\n",
    "    clf.fit(X_train_reduced_final, y_train_final)\n",
    "    y_pred_reduced = clf.predict(X_test_reduced)\n",
    "    acc_reduced = accuracy_score(y_test, y_pred_reduced)\n",
    "    cm_reduced = confusion_matrix(y_test, y_pred_reduced)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Full Features - Accuracy: {acc_full:.4f}, Confusion Matrix:\\n{cm_full}\")\n",
    "    print(f\"Reduced Features - Accuracy: {acc_reduced:.4f}, Confusion Matrix:\\n{cm_reduced}\")\n",
    "\n",
    "# Step 5: Challenge Yourself - Alternative Feature Reduction\n",
    "# Use Random Forest feature importance to select top 10 features\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_full_final, y_train_final)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_full.columns)\n",
    "top_features = importances.nlargest(10).index.tolist()  # Top 10 features\n",
    "print(\"\\nTop 10 features by Random Forest importance:\", top_features)\n",
    "\n",
    "# Create new reduced set with top features\n",
    "X_top = X_full[top_features]\n",
    "X_train_top = X_top.iloc[train_idx]  # Reuse train_idx from first split\n",
    "X_test_top = X_top.iloc[test_idx]    # Reuse test_idx from first split\n",
    "X_train_top_final = X_train_top.iloc[train_idx_2]  # Reuse train_idx_2 from second split\n",
    "X_val_top = X_train_top.iloc[val_idx]  # Reuse val_idx from second split\n",
    "\n",
    "print(\"\\nData split shapes (Top 10 features):\")\n",
    "print(\"Train:\", X_train_top_final.shape, \"Val:\", X_val_top.shape, \"Test:\", X_test_top.shape)\n",
    "\n",
    "# Train and evaluate classifiers on new reduced set\n",
    "print(\"\\nClassification Results with Top 10 Features:\")\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_top_final, y_train_final)\n",
    "    y_pred_top = clf.predict(X_test_top)\n",
    "    acc_top = accuracy_score(y_test, y_pred_top)\n",
    "    cm_top = confusion_matrix(y_test, y_pred_top)\n",
    "    print(f\"{name} - Accuracy: {acc_top:.4f}, Confusion Matrix:\\n{cm_top}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison: Look at accuracy scores above. Full features may perform better due to more information, \"\n",
    "      \"but reduced sets can achieve >94% accuracy with fewer features, improving efficiency.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
